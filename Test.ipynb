{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiagent Deep-Q learning\n",
    "This code is implemented in Tensorflow-2 and Python 3.6\n",
    "## Requirments\n",
    "Latest version of Tensorflow-2 can be downloaded from the [official website](https://www.tensorflow.org/install). Tensorflow-2 requires [CUDA 11](https://developer.nvidia.com/cuda-11.0-download-archive)\\\n",
    "Install other dependencies using the following line of code \n",
    "```console\n",
    "pip install --user -r requirements.txt\n",
    "```\n",
    "This package uses [openai/multiagent-particle-envs](https://github.com/openai/multiagent-particle-envs), to install it use the following code\n",
    "```console\n",
    "pip install -e /mape/\n",
    "```\n",
    "\n",
    "## Getting started\n",
    "For running a single agent DQN, use the following code*\n",
    "```console\n",
    "python main.py --net_mode 'DQN'\n",
    "```\n",
    "For multiagent indpendent DQN**,\n",
    "```console\n",
    "python main.py --net_mode 'DQN' --independent\n",
    "```\n",
    "For single agent DRQN, \n",
    "```console\n",
    "python main.py --net_mode 'DQN' --recurrent\n",
    "```\n",
    "For single agent dueling,\n",
    "```console\n",
    "python main.py --net_mode 'Dueling' --recurrent\n",
    "```\n",
    "\\* *Environment is set at multiagent by default. Change line 11 of /mape/multiagent/scenarios/simple_spread.py* \\\n",
    "\\** *Multiagent DQN will use recurrent network by default*\n",
    "\n",
    "## Instructions on modifying the game\n",
    "### Changing environment\n",
    "Add the preferred environment class and change code in lines 110 and 129 in *main.py* \\\n",
    "```python\n",
    "    env = MultiagentEnv(args.num_agents)\n",
    "    dqn.fit(env, args.num_samples, args.max_episode_length)\n",
    "\n",
    "```\n",
    "No further changes needed.\n",
    "### Increasing agents\n",
    "\n",
    "For increasing the number of agents follow as given below:\n",
    "1. **/mape/multiagent/scenarios/simple_spread.py**: Change the number of agents in line 11\n",
    "```\n",
    "self.num_agents = 2\n",
    "```\n",
    "Other less relevant changes which can be made is reducing agent and obstacle size as the evironment could clutter. These changes can be made in lines 21, 28, 33 and 40.\n",
    "\n",
    "2. **/deeprl_prj/idqn_keras.py**: Network size scales with increasing agents. Therefore it is necessary to modify lines 58 and 70, moreover the depth of policy layers can be increased in case of large policy size or number of agents. \\\n",
    "Add addtional networks in following the lines of code 166-169. Subseqeuent changes are to be made in calc_q_values, select_action and update_policy functions. Correct lines 406-407 depending on the policy size.\n",
    "\n",
    "\n",
    "3. **/deeprl_prj/core.py**: Change the policy shape on line 196.  A similar change is to be made in **/deeprl_prj/preprocesors.py**, line 31.\n",
    "4. While running the code use the flag '-num_agents' which is set default at 2.\n",
    "\n",
    "## Acknowledgements\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
